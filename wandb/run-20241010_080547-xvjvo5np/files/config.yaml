_wandb:
    value:
        cli_version: 0.18.3
        m:
            - "1": trainer/global_step
              "6":
                - 3
              "7": []
        python_version: 3.10.13
        t:
            "1":
                - 1
                - 9
                - 41
                - 55
            "2":
                - 1
                - 9
                - 41
                - 55
            "3":
                - 1
                - 7
                - 13
                - 16
                - 23
                - 55
                - 66
            "4": 3.10.13
            "5": 0.18.3
            "8":
                - 3
                - 5
            "12": 0.18.3
            "13": windows-amd64
data:
    value:
        eval_batch_size: 4
        num_workers: 5
        scale_aug: true
        test_year: "2014"
        train_batch_size: 8
        zipfile_path: data_crohme.zip
model:
    value:
        alpha: "1.0"
        beam_size: 10
        cross_coverage: true
        d_model: 256
        dc: 32
        dim_feedforward: 1024
        dropout: "0.3"
        early_stopping: false
        growth_rate: 24
        learning_rate: "0.08"
        max_len: 200
        nhead: 8
        num_decoder_layers: 1
        num_layers: 5
        patience: 20
        self_coverage: true
        temperature: "1.0"
seed_everything:
    value: 7
trainer:
    value:
        callbacks:
            - class_path: pytorch_lightning.callbacks.LearningRateMonitor
              init_args:
                logging_interval: epoch
            - class_path: pytorch_lightning.callbacks.ModelCheckpoint
              init_args:
                filename: '{epoch}-{step}-{val_ExpRate:.4f}'
                mode: max
                monitor: val_ExpRate
                save_top_k: 1
        check_val_every_n_epoch: 2
        checkpoint_callback: true
        default_root_dir: lightning_logs/version_0
        deterministic: true
        gpus: 1
        max_epochs: 300
        num_sanity_val_steps: 1
